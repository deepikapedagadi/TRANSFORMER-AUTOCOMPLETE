# TRANSFORMER-AUTOCOMPLETE
This project implements a from-scratch Transformer-based language model (Mini-GPT) for real-time next-token prediction and self-attention visualization using PyTorch and Streamlit. It predicts probable next tokens from input text and visualizes attention weights to interpret contextual dependencies.
